{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CESI HumanForYou\n",
    "\n",
    "L'entreprise de produits pharmaceutiques HumanForYou basée en Inde emploie environ 4000 personnes. Cependant, chaque année elle subit un turn-over d'environ 15% de ses employés nécessitant de retrouver des profils similaires sur le marché de l'emploi.\n",
    "\n",
    "La direction trouve que ce niveau de turn-over n'est pas bon pour l'entreprise car :\n",
    "\n",
    "* Les projets sur lesquels étaient les employés quittant la société prennent du retard ce qui nuit à la réputation de l'entreprise auprès de ses clients et partenaires.\n",
    "\n",
    "* Un service de ressources humaines de taille conséquente doit être conservé car il faut avoir les moyens de trouver les nouvelles recrues.\n",
    "\n",
    "* Du temps est perdu à l'arrivée des nouveaux employés car ils doivent très souvent être formés et ont besoin de temps pour devenir pleinement opérationnels dans leur nouvel environnement.\n",
    "\n",
    "Le direction fait donc appel à notre équipe, spécialistes de l'analyse de données, pour déterminer les facteurs ayant le plus d'influence sur ce taux de turn-over et lui proposer des modèles afin d'avoir des pistes d'amélioration pour donner à leurs employés l'envie de rester.\n",
    "\n",
    "### Table des matières\n",
    "\n",
    "1. [Préparation de l'environnement](#chapter1)\n",
    "    1. [Importation des librairies](#section_1_1)\n",
    "    2. [Importation des données](#Section_1_2)\n",
    "2. [Visualisation des données](#chapter2)\n",
    "    1. [Données du service des ressources humaines](#section_2_1)\n",
    "    2. [Dernière évaluation du manager](#section_2_2)\n",
    "    3. [Enquête qualité de vie au travail](#section_2_3)\n",
    "    4. [Horaires de travail](#section_2_4)\n",
    "3. [Transformation des données](#chapter3)\n",
    "    1. [Calcul des durées de travail](#section_3_1)\n",
    "    2. [Concaténation des données](#section_3_2)\n",
    "    3. [Ajout de valeur](#section_3_3)\n",
    "    4. [Suppression colonne](#section_3_4)\n",
    "    5. [Normalisation](#section_3_5)\n",
    "    6. [Standardisation](#section_3_6)\n",
    "    7. [Création des jeux de données ](#section_3_7)\n",
    "4. [Analyses statistiques](#chapter4)\n",
    "    1. [Analyse des données entrantes](#section_4_1)\n",
    "    2. [Analyse de l'attrition](#section_4_2)\n",
    "5. [Algorithmes](#chapter5)\n",
    "6. [Evaluation des modèles](#chapter6)\n",
    "\n",
    "\n",
    "<a id=\"chapter1\"></a>\n",
    "## Préparation de l'environnement\n",
    "\n",
    "<a id=\"section_1_1\"></a>\n",
    "### Importation des librairies\n",
    "\n",
    "Tout d'abord, nous devons importer toutes les bibliothèques que nous utiliserons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.random import default_rng\n",
    "from datetime import datetime\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, RandomizedSearchCV \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from numpy import argmax\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_1_2\"></a>\n",
    "### Importation des données\n",
    "\n",
    "Les données utilisées pour nos analyses proviennent de fichier CSV depuis Github et doivent être charger dans nos variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des données depuis Github \n",
    "general_url = \"https://raw.githubusercontent.com/anthonylorendeaux/CESI-IA-CSV/master/general_data.csv\"\n",
    "manager_url = \"https://raw.githubusercontent.com/anthonylorendeaux/CESI-IA-CSV/master/manager_survey_data.csv\"\n",
    "employee_url = \"https://raw.githubusercontent.com/anthonylorendeaux/CESI-IA-CSV/master/employee_survey_data.csv\"\n",
    "in_time_url = \"https://raw.githubusercontent.com/anthonylorendeaux/CESI-IA-CSV/master/in_time.csv\"\n",
    "out_time_url = \"https://raw.githubusercontent.com/anthonylorendeaux/CESI-IA-CSV/master/out_time.csv\"\n",
    "\n",
    "#Lecture des csv\n",
    "general_info_data = pd.read_csv(general_url)\n",
    "manager_survey_data = pd.read_csv(manager_url)\n",
    "employee_survey_data = pd.read_csv(employee_url)\n",
    "in_time_data = pd.read_csv(in_time_url)\n",
    "out_time_data = pd.read_csv(out_time_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chapter2\"></a>\n",
    "## Visualisation des données\n",
    "\n",
    "Dans un premier temps, nous regardons toutes les données que nous avons. Un certain nombre de données concernant les employés nous a donc été transmis par le service des ressources humaines ainsi que par des fiches d'évaluation.\n",
    "\n",
    "Les données ont été anonymisées : un employé de l'entreprise sera représenté par le même EmployeeID dans l'ensemble des fichiers qui suivent.\n",
    "\n",
    "<a id=\"section_2_1\"></a>\n",
    "### Données du service des ressources humaines\n",
    "\n",
    "Pour chaque employé, le service des ressources humaines vous confie les informations en sa possession :\n",
    "\n",
    "* Age : L'âge de l'employé en 2015.\n",
    "* Attrition : L'objet de notre étude, est-ce que l'employé a quitté l'entreprise durant l'année 2016 ?\n",
    "\n",
    "* BusinessTravel : A quel fréquence l'employé a été amené à se déplacer dans le cadre de son travail en 2015 ? (Non-Travel = jamais, Travel_Rarely= rarement, Travel_Frequently = fréquemment)\n",
    "\n",
    "* DistanceFromHome : Distance en km entre le logement de l'employé et l'entreprise.\n",
    "\n",
    "* Education : Niveau d'étude : 1=Avant College (équivalent niveau Bac), 2=College (équivalent Bac+2), 3=Bachelor (Bac+3), 4=Master (Bac+5) et 5=PhD (Thèse de doctorat).\n",
    "\n",
    "* EducationField : Domaine d'étude, matière principale\n",
    "\n",
    "* EmployeeCount : booléen à 1 si l'employé était compté dans les effectifs en 2015.\n",
    "\n",
    "* EmployeeId : l'identifiant d'un employé\n",
    "\n",
    "* Gender : Sexe de l'employé\n",
    "\n",
    "* JobLevel : Niveau hiérarchique dans l'entreprise de 1 à 5\n",
    "\n",
    "* JobRole : Métier dans l'entreprise\n",
    "\n",
    "* MaritalStatus : Statut marital du salarié (Célibataire, Marié ou Divorcé).\n",
    "\n",
    "* MonthlyIncome : Salaire brut en roupies par mois\n",
    "\n",
    "* NumCompaniesWorked : Nombre d'entreprises pour lequel le salarié a travaillé avant de rejoindre HumanForYou.\n",
    "\n",
    "* Over18 : Est-ce que le salarié a plus de 18 ans ou non ?\n",
    "\n",
    "* PercentSalaryHike : % d'augmentation du salaire en 2015.\n",
    "\n",
    "* StandardHours : Nombre d'heures par jour dans le contrat du salarié.\n",
    "\n",
    "* StockOptionLevel : Niveau d'investissement en actions de l'entreprise par le salarié.\n",
    "\n",
    "* TotalWorkingYears : Nombre d'années d'expérience en entreprise du salarié pour le même type de poste.\n",
    "\n",
    "* TrainingTimesLastYear : Nombre de jours de formation en 2015\n",
    "\n",
    "* YearsAtCompany : Ancienneté dans l'entreprise\n",
    "\n",
    "* YearsSinceLastPromotion : Nombre d'années depuis la dernière augmentation individuelle\n",
    "\n",
    "* YearsWithCurrentManager : Nombre d'années de collaboration sous la responsabilité du manager actuel de l'employé.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_info_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est aussi intéressant de connaitre le type des variables qui composent le fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of general_info_data :',general_info_data.shape)\n",
    "general_info_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_2_2\"></a>\n",
    "### Dernière évaluation du manager\n",
    "\n",
    "Ce fichier contient la dernière évaluation de chaque employé faite pas son manager en février 2015.\n",
    "\n",
    "Il contient les données suivantes :\n",
    "\n",
    "* L'identifiant de l'employé : EmployeeID\n",
    "\n",
    "* Une évaluation de son implication dans son travail notée 1 ('Faible'), 2 (\"Moyenne\"), 3 (\"Importante\") ou 4 (\"Très importante\") : JobInvolvement\n",
    "\n",
    "* Une évaluation de son niveau de performance annuel pour l'entreprise notée 1 (\"Faible\"), 2 (\"Bon\"), 3 (\"Excellent\") ou 4 (\"Au delà des attentes\") : PerformanceRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_survey_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_survey_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_2_3\"></a>\n",
    "### Enquête qualité de vie au travail\n",
    "\n",
    "Ce fichier provient d'une enquête soumise aux employés en juin 2015 par le service RH pour avoir un retour concernant leur qualité de vie au travail.\n",
    "\n",
    "Une organisation avait été mise en place pour que chacun puisse répondre à ce questionnaire sur son lieu de travail en concertation avec les managers mais il n'y avait pas d'obligation.\n",
    "\n",
    "Les employés devaient répondre à 3 questions sur le niveau de satisfaction concernant :\n",
    "\n",
    "* L'environnement de travail, noté 1 (\"Faible\"), 2 (\"Moyen\"), 3 (\"Élevé\") ou 4 (\"Très élevé\") : EnvironmentSatisfaction\n",
    "\n",
    "* Son travail, noté de 1 à 4 comme précédemment : JobSatisfaction\n",
    "\n",
    "* Son équilibre entre vie professionnelle et vie privée, noté 1 (\"Mauvais\"), 2 (\"Satisfaisant\"), 3 (\"Très satisfaisant\") ou 4 (\"Excellent\") : WorkLifeBalance\n",
    "\n",
    "Lorsque un employé n'a pas répondu à une question, le texte \"NA\" apparaît à la place de la note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_survey_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_survey_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_2_4\"></a>\n",
    "### Horaires de travail\n",
    "\n",
    "Des badgeuses sont installées et utilisées dans l'entreprise depuis quelques années. Il a été jugé opportun par la direction de nous transmettre les horaires d'entrée et de sortie des employés sur une période de l'année choisie représentative d'une activité moyenne pour l'ensemble des services.\n",
    "\n",
    "Nous avons donc 2 fichiers traçants les horaires d'arrivée à leur poste et de départ de leur poste de l'ensemble des employés par date sur une période allant du 1er janvier au 31 décembre 2015.\n",
    "\n",
    "Données d'arrivée des employées:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_time_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_time_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données de départ des employées: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_time_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_time_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chapter3\"></a>\n",
    "## Transformation des données\n",
    "\n",
    "Plusieurs données ne peuvent pas être exploiter en l'état, il faut donc trier et retravailler les données.\n",
    "\n",
    "<a id=\"section_3_1\"></a>\n",
    "### Calcul des durées de travail\n",
    "\n",
    "Les données de temps ne sont pas exploitables sous cette forme, il faut donc les tranformer.\n",
    "\n",
    "Avoir des heures d'entrées et de sortie de nos employés n'est pas très significatifs, c'est pour cela que nous remplaçons toutes les valeurs par la moyenne de temps de travail de chaque employé.\n",
    "\n",
    "Cependant : \n",
    "* Les dates sont stockées en tant que chaine de caractère et il est compliqué de les exploiter.\n",
    "* Cetaines données valent \"NaN\", ce qui veut dire qu'un employé a été absent au travail\n",
    "\n",
    "Pour remédier à ça, nous transformons les données en objet Datetime. De plus lorsqu'un employé est absent au travail, son temps moyen de travail est de 0 donc nous remplaçons les NaN par 0.\n",
    "\n",
    "Avant de mettre en place nos changements, nous devons renommer la colonne (sans nom) qui correspond aux IDs des employés. Cette actions est prise puisque dans le csv nous avons le même nombre de ligne que sur les autres csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommage des colonnes sans nom de nos csv in et out\n",
    "in_time_data.rename(columns={\"Unnamed: 0\": \"EmployeeID\"}, inplace=True)\n",
    "in_time_data.set_index('EmployeeID', inplace=True)\n",
    "in_time_data\n",
    "out_time_data.rename(columns={\"Unnamed: 0\": \"EmployeeID\"}, inplace=True)\n",
    "out_time_data.set_index('EmployeeID', inplace=True)\n",
    "\n",
    "# Suppression des colonnesoù l'employée est absent (valeur NaN)\n",
    "in_time_data=in_time_data.dropna(axis=1,how='all')\n",
    "out_time_data=out_time_data.dropna(axis=1,how='all')\n",
    "\n",
    "# Remplacement des NaN par 0\n",
    "in_time_data.fillna(0, inplace=True)\n",
    "out_time_data.fillna(0, inplace=True)\n",
    "\n",
    "out_time_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On transforme nos chaines de caractère en objets datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in in_time_data.columns:\n",
    "    in_time_data[date]=pd.to_datetime(in_time_data[date])\n",
    "    out_time_data[date]=pd.to_datetime(out_time_data[date])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule dans un nouveau dataset le nombre d'heure passé qu'en employé passe au travail par jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_work_per_day=pd.DataFrame()\n",
    "\n",
    "cols=in_time_data.columns\n",
    "for col in cols:\n",
    "    time_work_per_day[col]=((out_time_data[col] - in_time_data[col]).dt.total_seconds() /3600)\n",
    "\n",
    "time_work_per_day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ajoutons ensuite une colonne représentant : \n",
    "* La moyenne de temps passsé au travail par employé sur l'année 2015 \n",
    "* Le nombre d'absences au travail par employé durant l'année 2015 \n",
    "\n",
    "Les autres colonnes sont ensuite supprimées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_work_per_day['MeanTimeWorkOverYear2015']=round(time_work_per_day.astype(int).mean(axis=1),2)\n",
    "time_work_per_day['absences_par_jour']=(time_work_per_day == 0).astype(int).sum(axis=1)\n",
    "time_work_per_day = time_work_per_day.drop((time_work_per_day.columns[0:-2]), axis = 1)\n",
    "time_work_per_day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_3_2\"></a>\n",
    "### Concaténation des données\n",
    "\n",
    "Pour la suite des analyses, nous allons rassembler toutes les données sur une même variable. Comme sur chaque csv, l'ID des employées est inscrit, il est facile de concater les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_time_csv = general_info_data.merge(time_work_per_day, on='EmployeeID')\n",
    "concat_manager_csv = concat_time_csv.merge(manager_survey_data, on='EmployeeID')\n",
    "temp_concat = concat_manager_csv.merge(employee_survey_data, on='EmployeeID')\n",
    "temp_concat = temp_concat.set_index('EmployeeID')\n",
    "temp_concat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide des informations précédentes, nous remarquons plusieurs choses:\n",
    "* Tous les champs ne possèdent pas le même nombre de tuples\n",
    "* Les champs ayants un type objet corréspondent à des variables qualitatives\n",
    "\n",
    "Pour la suite des analyses, nous devons d'abord harmoniser nos données pour ne plus avoir les deux remarques précédentes.\n",
    "\n",
    "<a id=\"section_3_3\"></a>\n",
    "### Ajout de valeur\n",
    "\n",
    "Pour palier au manque de certaines données, nous comblons les valeurs manquantes par la valeur médiane de ses champs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de champs avec des valeurs vides\n",
    "final_data = temp_concat.copy()\n",
    "final_data[final_data.columns[final_data.isnull().any()]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des valeurs médianes\n",
    "final_data.fillna(round(final_data.median()),inplace=True)\n",
    "final_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_3_4\"></a>\n",
    "### Suppression de colonne\n",
    "\n",
    "Il est important de vérifier qu'il n'y est pas des champs avec valeur similaire partout. Cela signifit que l'information n'est pas pertinente et qu'elle peut être supprimer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide de ce tableau, nous remarquons qu'il existe des champs à valeur unique :\n",
    "- EmployeeCount : la valeur min et max est égale à 1\n",
    "- Over18 : cette varible possède une unique valeur (unique = 1)\n",
    "- StandardHours : la valeur min et max est égale à 8\n",
    "\n",
    "Nous supprimons donc ces champs pour la suite des travaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.drop(['EmployeeCount', 'Over18', 'StandardHours'], axis=1, inplace = True)\n",
    "final_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_3_5\"></a>\n",
    "### Normalisation\n",
    "\n",
    "Comme dans la suite de ce projet nous devrons utiliser des algorithmes de machine learning, il est important de prendre en compte les recommandations de ses derniers. Comme de nombreux algorithmes d'apprentissage automatique ne peuvent pas fonctionner directement sur des données qualitatives, nous devons prévoir un ensemble de données dont toutes les variables d'entrée et les variables de sortie soient numériques.\n",
    "\n",
    "Afin de n'avoir que des données quantitatives, nous utilisons le One-hot Encoding. Quand une variables n'est pas ordinale, cette solution va créer des variables supplémentaires dans le jeu de donnée pour représenter chacune des catégories.\n",
    "\n",
    "Les champs concernés sont : \"BusinessTravel\",\"Department\",\"EducationField\", \"JobRole\",\"MaritalStatus\", \"Gender\".\n",
    "Et vont être remplacés par : \n",
    "* BusinessTravel_Non-Travel\n",
    "* BusinessTravel_Travel_Frequently\n",
    "* BusinessTravel_Travel_Rarely\n",
    "* Department_Human Resources\n",
    "* Department_Research & Development\n",
    "* Department_Sales\n",
    "* EducationField_Human Resources\n",
    "* EducationField_Life Sciences\n",
    "* EducationField_Marketing\n",
    "* EducationField_Medical\n",
    "* EducationField_Other\n",
    "* EducationField_Technical Degree\n",
    "* JobRole_Healthcare Representative\n",
    "* JobRole_Human Resources\n",
    "* JobRole_Manufacturing Director\t\n",
    "* JobRole_Research Director\t\n",
    "* JobRole_Research Scientist\t\n",
    "* JobRole_Sales Executive\t\n",
    "* JobRole_Sales Representative\t\n",
    "* MaritalStatus_Divorced\t\n",
    "* MaritalStatus_Married\t\n",
    "* MaritalStatus_Single\t\n",
    "* Gender_Female\t\n",
    "* Gender_Male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot Encoding\n",
    "final_data2 = pd.get_dummies(final_data, \n",
    "prefix=[\"BusinessTravel\",\"Department\",\"EducationField\", \"JobRole\",\"MaritalStatus\", \"Gender\"],\n",
    "columns=[\"BusinessTravel\",\"Department\",\"EducationField\", \"JobRole\",\"MaritalStatus\", \"Gender\"])\n",
    "final_data2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_3_6\"></a>\n",
    "\n",
    "### Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Suppression de la colonne Attrition\n",
    "final_data3 = final_data2.drop(\"Attrition\",axis=1)\n",
    "\n",
    "# Normalisation des données en utilisant le z-score \n",
    "norm = StandardScaler().fit_transform(final_data3)\n",
    "final_data3 = pd.DataFrame(norm, columns=final_data3.columns)\n",
    "final_data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression cols\n",
    "cols_pas_ethic = [\"Gender_Male\", \"Gender_Female\", \"YearsWithCurrManager\", \"PerformanceRating\", \"Age\", \"MaritalStatus_Single\", \"MaritalStatus_Married\", \"MaritalStatus_Divorced\"]\n",
    "cols_big_corr = [\"Department_Sales\", \"BusinessTravel_Travel_Rarely\"]\n",
    "\n",
    "final_data4 = final_data3.drop(cols_pas_ethic,axis=1)\n",
    "final_data4 = final_data4.drop(cols_big_corr,axis=1)\n",
    "final_data4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_3_7\"></a>\n",
    "\n",
    "### Création des jeux de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définitions de nos variables X et y\n",
    "X = final_data4\n",
    "y = final_data2[\"Attrition\"]\n",
    "\n",
    "\n",
    "seed =42\n",
    "test_size = 0.2\n",
    "# Fixer un seed pour avoir les mêmes résultats à chaque essai\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Séparer les données d'entrainement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la suite de cette section, nous avons deux ensembles de données nettoyés et complets:\n",
    "* final_data :  données qualitatives et quantitatives\n",
    "* final_data2 : données uniquement quantitatives \n",
    "* final_data3 : données uniquement quantitatives et standardisées \n",
    "\n",
    "\n",
    "<a id=\"chapter4\"></a>\n",
    "\n",
    "## Analyses statistiques\n",
    "\n",
    "Nous séparons nos données en deux parties : \n",
    "-  Les valeurs entrantes qui correspondent à tous les paramêtres qui possèdent différentes valeurs (Ex: DistanceFromHome, Education, Age ...)\n",
    "- La valeur de sortie (variable 'Attrition') qui permet de savoir si l'employée à quitté ou non l'entreprise l'année suivante.\n",
    "\n",
    "Pour cela, nous allons analyser nos deux groupes.\n",
    "\n",
    "<a id=\"section_4_1\"></a>\n",
    "### Analyste des données entrantes\n",
    "\n",
    "#### Histogramme \n",
    "\n",
    "Concernant les données quantitatives, nous affichons nos valeurs de façon visuelle afin d'avoir un meilleur appercu de l'allure de nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrélation des données\n",
    "\n",
    "Maintenant nous voulons voir si nos valeurs possèdent des liens entre-elles. Pour cela nous utilisons une matrice de corrélation. Elle permet d'évaluer la dépendence entre plusieurs variables en même temps. Le résultat est une table contenant les coefficients de corrélation entre chaque variable et les autres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corrélation\n",
    "corr_matrix = final_data2.corr(method=\"pearson\")\n",
    "\n",
    "plt.figure(figsize=(40,30))\n",
    "sns.heatmap(corr_matrix, annot= True, cbar=True, cmap=\"Blues_r\")\n",
    "plt.show()\n",
    "\n",
    "corr_matrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "corr_matrix = corr_matrix.unstack().reset_index()\n",
    "corr_matrix.columns = [\"feature1\", \"feature2\", \"Correlation\"]\n",
    "corr_matrix.dropna(subset = ['Correlation'], inplace = True)\n",
    "corr_matrix['Correlation'] = round(corr_matrix['Correlation'], 2)\n",
    "corr_matrix['Correlation'] = abs(corr_matrix['Correlation'])\n",
    "corr_matrix = corr_matrix.sort_values(by = 'Correlation', ascending = False)\n",
    "value_high_corr = corr_matrix[corr_matrix['Correlation']>0.7]\n",
    "\n",
    "value_high_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_4_2\"></a>\n",
    "\n",
    "### Analyse de l'attrition\n",
    "\n",
    "Maintenant que nous avons analyser les paramêtres indépendament de l'attrition, nous allons regarder leur influence sur cette dernière.\n",
    "\n",
    "#### Courbes de densité superposées\n",
    "\n",
    "A l'aide des courbes de densité superposées, nous regardons la répartition des valeurs quantitatives pour chaque variables en fonction de l'attrition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_chart(df,feature):\n",
    "    ax=sns.kdeplot(df[df['Attrition']=='Yes'][feature],\n",
    "             shade=True,label='Attrition = Yes')\n",
    "    ax=sns.kdeplot(df[df['Attrition']=='No'][feature],\n",
    "                 shade=True,label='Attrition = No')\n",
    "    ax.legend()\n",
    "\n",
    "cols = ['DistanceFromHome','MonthlyIncome','TotalWorkingYears','TrainingTimesLastYear', 'YearsAtCompany', 'YearsSinceLastPromotion','PercentSalaryHike','YearsWithCurrManager','MeanTimeWorkOverYear2015','absences_par_jour', \"Age\"]\n",
    "\n",
    "plt.figure(figsize=(40,30))\n",
    "for i, col in enumerate(cols):\n",
    "    ax = plt.subplot(4, 3, i+1)\n",
    "    area_chart(final_data, col)\n",
    "    # for bar in plots.patches:\n",
    "    #     percentage = '{:.1f}%'.format(100 * bar.get_height() / len(final_data[col]))\n",
    "    #     x = bar.get_x() + bar.get_width() / 2\n",
    "    #     y = bar.get_height()\n",
    "    #     plots.annotate(percentage, (x, y),ha='center',va='center',size=15, xytext=(0, 3),\n",
    "    #                textcoords='offset points')\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant comme nous voyons peu de différence à l'oeil nu, nous allons dans un deuxième temps des boites à moustache.\n",
    "\n",
    "#### Boîtes à moustache\n",
    "\n",
    "A l'aide de boites à moustache, nous regardons la répartition des valeurs quantitatives pour chaque variables en fonction de l'attrition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['DistanceFromHome','MonthlyIncome','TotalWorkingYears','TrainingTimesLastYear', 'YearsAtCompany', 'YearsSinceLastPromotion','PercentSalaryHike','YearsWithCurrManager','MeanTimeWorkOverYear2015','absences_par_jour']\n",
    "\n",
    "plot_rows=2\n",
    "plot_cols=5\n",
    "fig = make_subplots(rows=plot_rows, cols=plot_cols,subplot_titles=(cols))\n",
    "\n",
    "col_i = 0\n",
    "for i in range(1, plot_rows + 1):\n",
    "    for j in range(1, plot_cols + 1):\n",
    "        for t in px.box(final_data, x=\"Attrition\", y=cols[col_i]).data:\n",
    "            fig.add_trace(t,row=i, col=j)\n",
    "\n",
    "        col_i=col_i+1\n",
    "fig.update_layout(height=600, width=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogrammes\n",
    "\n",
    "A l'aide des histogrammes, nous regardons la répartition des valeurs qualitatives pour chaque variables en fonction de l'attrition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['BusinessTravel', 'Department', 'Education', 'EducationField', 'Gender', 'JobLevel', 'JobRole', 'MaritalStatus', 'StockOptionLevel', 'JobInvolvement', 'PerformanceRating', 'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance']\n",
    "\n",
    "plt.figure(figsize=(40,20))\n",
    "for i, col in enumerate(cols):\n",
    "    ax = plt.subplot(3, 5, i+1)\n",
    "    plots = sns.histplot(data=final_data, x=col, ax=ax, hue=final_data['Attrition'], multiple='dodge', shrink=0.8)\n",
    "\n",
    "\n",
    "    # From axis.patches get bar lengths\n",
    "    get_bars = plots.patches\n",
    "    half_bar_length = int(len(get_bars)/2)\n",
    "    bar_left = get_bars[:half_bar_length]\n",
    "    bar_right = get_bars[half_bar_length:]\n",
    "    \n",
    "    # Place %employees on top of each bar\n",
    "    for L, R in zip(bar_left, bar_right):\n",
    "        left_height = L.get_height()\n",
    "        right_height = R.get_height()\n",
    "        length_total = left_height + right_height\n",
    "\n",
    "        # place calculated employee percentage on top of each bar\n",
    "        ax.text(L.get_x() + L.get_width()/2., left_height + 30, '{0:.0%}'.format(left_height/length_total), ha=\"center\")\n",
    "        ax.text(R.get_x() + R.get_width()/2., right_height + 30, '{0:.0%}'.format(right_height/length_total), ha=\"center\")\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chapter5\"></a>\n",
    "## Algorithmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_label = y_train.copy()\n",
    "y_train_label.where(y_train_label == \"Yes\", 0, inplace=True)\n",
    "y_train_label.where(y_train_label == 0, 1, inplace=True)\n",
    "y_train_label = y_train_label.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_label = y_test.copy()\n",
    "y_test_label.where(y_test_label == \"Yes\", 0, inplace=True)\n",
    "y_test_label.where(y_test_label == 0, 1, inplace=True)\n",
    "y_test_label = y_test_label.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listes des algorithmes  \n",
    "models = {\"Logistic Regression\": LogisticRegression(),\n",
    "          \"KNN\": KNeighborsClassifier(),\n",
    "          \"Random Forest\": RandomForestClassifier(),\n",
    "          \"Perceptron\": Perceptron(),\n",
    "          \"Descente de gradient\": SGDClassifier()}\n",
    "\n",
    "# fonction appliquant un fit et score sur chacun des modeles\n",
    "def fit_and_score(models, X_train, y_train_label):\n",
    "    \n",
    "    # Fixer un seed pour avoir les mêmes résultats à chaque essai \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Dictionnaire pour sauvegarder les scores\n",
    "    fitted_model = {}\n",
    "    for name, model in models.items():\n",
    "        # Fit le modèle\n",
    "        model.fit(X_train, y_train_label)\n",
    "        fitted_model[name] = model\n",
    "        \n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model = fit_and_score(models=models, X_train=X_train, y_train_label=y_train_label)\n",
    "fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comparaison des résultats obtenus\n",
    "# model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\n",
    "# model_compare.T.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chapter5\"></a>\n",
    "\n",
    "## Evaluation des modèles\n",
    "\n",
    "<a id=\"section_5_1\"></a>\n",
    "### Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction appliquant xxxxxxxxxxxxxxxxxxx\n",
    "def evaluation_accuracy(models, X, y):\n",
    "    \n",
    "    tabl = pd.DataFrame(columns=['Algorithme', 'Accuracy Score Train', 'Accuracy Score Test', 'Différence'])\n",
    "    tabl.style.hide_index()\n",
    "\n",
    "    i = 0   \n",
    "    for name, model in models.items():\n",
    "        # Evaluer le modèle selon le score de chaque algorithme\n",
    "        cross_val = (cross_val_score(model, X, y, cv=10, scoring='accuracy')).mean()\n",
    "        \n",
    "        #\n",
    "        y_pred = model.predict(X)\n",
    "        accuracy_test = accuracy_score(y, y_pred)\n",
    "        \n",
    "        diff = accuracy_test - cross_val\n",
    "        \n",
    "        tabl.loc[i] = [name, cross_val, accuracy_test, diff]\n",
    "        i=i+1\n",
    "        \n",
    "        #confusion matrix\n",
    "        cm = metrics.confusion_matrix(y, y_pred)\n",
    "        \n",
    "        #Visualize the confusion matrix\n",
    "        plt.figure(figsize=(9,9))\n",
    "        sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "        plt.ylabel('Actual Class');\n",
    "        plt.xlabel('Predicted Class');\n",
    "        all_sample_title = \"Test Accuracy: %0.2f\" % (cross_val)\n",
    "        plt.title(all_sample_title, size = 15);\n",
    "\n",
    "\n",
    "    return tabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_accuracy(fitted_model, X_train, y_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning\n",
    "\n",
    "#Données de paramètres\n",
    "\n",
    "param_reglog = [\n",
    "    {'solver': ['newton-cg', 'liblinear'],\n",
    "    'penalty': ['none', 'l1', 'l2']}\n",
    "]\n",
    "\n",
    "param_knn = [\n",
    "    {'n_neighbors' : range(1,10),\n",
    "    'weights' : ['uniform', 'distance'],\n",
    "    'metric' : ['euclidean', 'manhattan']}\n",
    "]\n",
    "\n",
    "param_random_forest = [\n",
    "    {'n_estimators' : [10, 50, 100, 200],\n",
    "    'max_depth' : [10, 20, 40, 50],\n",
    "    'max_features': [1,10,30,40],\n",
    "    'bootstrap': [True, False]}\n",
    "]\n",
    "\n",
    "param_perceptron = [\n",
    "    {'penalty' : ['l1', 'l2'],\n",
    "    'fit_intercept': [True, False],\n",
    "    'max_iter': [20, 50, 70, 100]}\n",
    "]\n",
    "\n",
    "param_descente = [\n",
    "    {'learning_rate': ['optimal'],\n",
    "    'early_stopping': [True],\n",
    "    'validation_fraction': [0.1, 0.3, 0.5],\n",
    "    'loss': ['log', 'perceptron'],\n",
    "    'penalty': ['l1', 'l2']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_method(gs_cv, X, y):\n",
    "    gs_cv.fit(X, y)\n",
    "    gs_cv_best_score = gs_cv.best_score_\n",
    "    gs_cv_best_param = gs_cv.best_params_\n",
    "    gs_cv_best_estimator = gs_cv.best_estimator_\n",
    "    gs_cv_score = cross_val_score(gs_cv_best_estimator, X, y, cv=10).mean()\n",
    "    \n",
    "    return gs_cv_best_score, gs_cv_best_param, gs_cv_best_estimator, gs_cv_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "gs_cv_reglog = GridSearchCV(LogisticRegression(), param_reglog, cv=10,return_train_score=False, verbose=0)\n",
    "\n",
    "# Jeu de test\n",
    "reglog_best_score_test, reglog_best_param_test, reglog_best_estimator_test, cv_score_reglog_test =  fit_method(gs_cv_reglog, X_test, y_test_label)\n",
    "print('reglog_best_score_test:',reglog_best_score_test)\n",
    "\n",
    "# Jeu d'entrainement\n",
    "reglog_best_score_train, reglog_best_param_train, reglog_best_estimator_train, cv_score_reglog_train =  fit_method(gs_cv_reglog, X_train, y_train_label)\n",
    "print('reglog_best_score_train:',reglog_best_score_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNNeighboors hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "gs_cv_knn = GridSearchCV(KNeighborsClassifier(), param_knn, cv=10,return_train_score=False, verbose=0)\n",
    "\n",
    "# Jeu de test\n",
    "knn_best_score_test, knn_best_param_test, knn_best_estimator_test, cv_score_knn_test =  fit_method(gs_cv_knn, X_test, y_test_label)\n",
    "print('knn_best_score_test:', knn_best_score_test)\n",
    "\n",
    "# Jeu d'entrainement\n",
    "knn_best_score_train, knn_best_param_train, knn_best_estimator_train, cv_score_knn_train =  fit_method(gs_cv_knn, X_train, y_train_label)\n",
    "print('knn_best_score_train:', knn_best_score_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "gs_cv_rf = GridSearchCV(RandomForestClassifier(), param_random_forest, cv=10,return_train_score=False, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Jeu de test\n",
    "rf_best_score_test, rf_best_param_test, rf_best_estimator_test, cv_score_rf_test =  fit_method(gs_cv_rf, X_test, y_test_label)\n",
    "print('rf_best_score_test:', rf_best_score_test)\n",
    "\n",
    "# Jeu d'entrainement\n",
    "rf_best_score_train, rf_best_param_train, rf_best_estimator_train, cv_score_rf_train =  fit_method(gs_cv_rf, X_train, y_train_label)\n",
    "print('rf_best_score_train:', rf_best_score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "gs_cv_perceptron = GridSearchCV(Perceptron(), param_perceptron, cv=10,return_train_score=False, verbose=0)\n",
    "\n",
    "# Jeu de test\n",
    "perceptron_best_score_test, perceptron_best_param_test, perceptron_best_estimator_test, cv_score_perceptron_test =  fit_method(gs_cv_perceptron, X_test, y_test_label)\n",
    "print('perceptron_best_score_test:', perceptron_best_score_test)\n",
    "\n",
    "# Jeu d'entrainement\n",
    "perceptron_best_score_train, perceptron_best_param_train, perceptron_best_estimator_train, cv_score_perceptron_train =  fit_method(gs_cv_perceptron, X_train, y_train_label)\n",
    "print('perceptron_best_score_train:', perceptron_best_score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descente de Gradient Stochastique hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "gs_cv_dgs = GridSearchCV(SGDClassifier(), param_descente, cv=10,return_train_score=False, verbose=0)\n",
    "\n",
    "# Jeu de test\n",
    "dgs_best_score_test, dgs_best_param_test, dgs_best_estimator_test, cv_score_dgs_test =  fit_method(gs_cv_dgs, X_test, y_test_label)\n",
    "print('dgs_best_score_test:', dgs_best_score_test)\n",
    "\n",
    "# Jeu d'entrainement\n",
    "dgs_best_score_train, dgs_best_param_train, dgs_best_estimator_train, cv_score_dgs_train =  fit_method(gs_cv_dgs, X_train, y_train_label)\n",
    "print('dgs_best_score_train:', dgs_best_score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_param = {\n",
    "    'Algorithmes': [\"Logisitic Regression\", \"KNN\", \"RandomForest\", \"Perceptron\", \"Descente de gradient stochastique\"],\n",
    "    'Best_param': [reglog_best_param_train, knn_best_param_train, rf_best_param_train, perceptron_best_param_train, dgs_best_param_train],\n",
    "    'Best_estimator': [reglog_best_estimator_train, knn_best_estimator_train, rf_best_estimator_train, perceptron_best_estimator_train, dgs_best_estimator_train],\n",
    "    'Cross-Validation score Train': [cv_score_reglog_train, cv_score_knn_train, cv_score_rf_train, cv_score_perceptron_train, cv_score_dgs_train]\n",
    "}\n",
    "hyperparameter = pd.DataFrame(data=data_param)\n",
    "hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall et précision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définie ici nos fonctions pour plot et de transfo\n",
    "# Convertir une liste avec des 'No' & 'Yes' en 0 & 1 liste\n",
    "def transform_binary(table):\n",
    "    y_pred_binary = []\n",
    "    \n",
    "    for item in table:\n",
    "        if item == 'No':\n",
    "            y_pred_binary.append(0)\n",
    "        else:\n",
    "            y_pred_binary.append(1)\n",
    "    return y_pred_binary\n",
    "\n",
    "# Plot courbes de précision et recall vs Threshold\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b-\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.legend(loc=\"upper left\", fontsize=16)\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "# Plot courbes de précision et recall\n",
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"k-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "\n",
    "# Plot de la courbe de Roc\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiche les informations pour un modèle donnée\n",
    "def displayInfo_by_model(estimator_train, estimator_test, X_train, y_true_train, X_test, y_true_test, method, xlim_x, xlim_y, ylim_x, ylim_y ):\n",
    "    # Fix random\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Print les infos de base (précision, recall, f1...)\n",
    "    y_pred_train = cross_val_predict(estimator_train, X_train, y_true_train, cv=7)\n",
    "    precision_train = precision_score(y_true_train, y_pred_train)\n",
    "    recall_train = recall_score(y_true_train, y_pred_train)\n",
    "    f1_train = f1_score(y_true_train, y_pred_train)\n",
    "    \n",
    "    y_pred_test = cross_val_predict(estimator_test, X_test, y_true_test, cv=7)\n",
    "    precision_test = precision_score(y_true_test, y_pred_test)\n",
    "    recall_test = recall_score(y_true_test, y_pred_test)\n",
    "    f1_test =  f1_score(y_true_test, y_pred_test)\n",
    "    \n",
    "    data_train_test = {\n",
    "    'Analyse': [\"precision_score\", \"recall_score\", \"f1_score\"],\n",
    "    \"Jeu d'entrainement\": [precision_train, recall_train, f1_train],\n",
    "    \"Jeu de test\": [precision_test, recall_test, f1_test],\n",
    "    'Différence': [precision_train - precision_test, recall_train - recall_test, f1_train - f1_test]\n",
    "    }\n",
    "        \n",
    "    result_train_test = pd.DataFrame(data=data_train_test)\n",
    "    display(result_train_test)\n",
    "    \n",
    "    \n",
    "    # Afficher courbes\n",
    "    y_scores = cross_val_predict(estimator_train, X_train, y_true_train, cv=7,method=method)\n",
    "    if(method == 'predict' or method == 'predict_proba'):\n",
    "        y_scores = y_scores[:,1]\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true_train, y_scores)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "    plt.xlim([xlim_x,xlim_y])\n",
    "    plt.ylim([ylim_x,ylim_y])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plot_precision_vs_recall(precisions, recalls)\n",
    "    plt.show()   \n",
    "\n",
    "    fscore = (2 * precisions * recalls) / (precisions + recalls)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true_train, y_scores)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plot_roc_curve(fpr, tpr)\n",
    "    plt.show()\n",
    "    \n",
    "    return fpr, tpr, fscore, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_label = y_test.copy()\n",
    "y_test_label.where(y_test_label == \"Yes\", 0, inplace=True)\n",
    "y_test_label.where(y_test_label == 0, 1, inplace=True)\n",
    "y_test_label = y_test_label.astype(np.int64)\n",
    "y_test_label_true = (y_test_label == 1)\n",
    "\n",
    "y_train_label = y_train.copy()\n",
    "y_train_label.where(y_train_label == \"Yes\", 0, inplace=True)\n",
    "y_train_label.where(y_train_label == 0, 1, inplace=True)\n",
    "y_train_label = y_train_label.astype(np.int64)\n",
    "y_train_label = (y_train_label == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_reglog, tpr_reglog, fscore_reglog, thresholds_reglog = displayInfo_by_model(reglog_best_estimator_train, reglog_best_estimator_test, X_train, y_train_label, X_test, y_test_label_true,\"decision_function\",-2,0,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_knn, tpr_knn, fscore_knn, thresholds_knn = displayInfo_by_model(knn_best_estimator_train, knn_best_estimator_test, X_train, y_train_label, X_test, y_test_label_true,\"predict_proba\", 0.25,0.45,0.8,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_rf, tpr_rf, fscore_rf, thresholds_rf = displayInfo_by_model(rf_best_estimator_train, rf_best_estimator_test, X_train, y_train_label, X_test, y_test_label_true,\"predict_proba\", 0.1,0.6,0.8,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_perceptron, tpr_perceptron, fscore_perceptron, thresholds_perceptron = displayInfo_by_model(perceptron_best_estimator_train, perceptron_best_estimator_test, X_train, y_train_label, X_test, y_test_label_true,\"decision_function\", -10,10, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descente de gradient stochastique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_dgs, tpr_dgs, fscore_dgs, thresholds_dgs = displayInfo_by_model(dgs_best_estimator_train, dgs_best_estimator_test, X_train, y_train_label, X_test, y_test_label_true,\"decision_function\", -2, 0, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr_rf, tpr_rf, \"Random Forest\")\n",
    "plot_roc_curve(fpr_knn, tpr_knn, \"KNN\")\n",
    "plot_roc_curve(fpr_reglog, tpr_reglog, \"Logistic Regression\")\n",
    "plot_roc_curve(fpr_perceptron, tpr_perceptron, \"Perceptron\")\n",
    "plot_roc_curve(fpr_dgs, tpr_dgs, \"Descente de Gradient Stochastique\")\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def percentage(unique, counts):\n",
    "    return (counts[1]/(counts[0]+counts[1]))*100\n",
    "\n",
    "ix = argmax(fscore_rf)\n",
    "\n",
    "final_model = rf_best_estimator_train\n",
    "final_threshold = thresholds_rf[ix]\n",
    "\n",
    "final_prediction = final_model.predict_proba(X_train)[:, 1]\n",
    "final_prediction_set = np.where(final_prediction < final_threshold, 0, 1)\n",
    "\n",
    "unique, counts = np.unique(final_prediction_set, return_counts=True)\n",
    "dict(zip(unique, counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_estimator_train.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_dict = dict(sorted(zip(X.columns, list(rf_best_estimator_train.feature_importances_)), reverse=True))\n",
    "feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(feature_dict, index=[0])\n",
    "feature_df.T.plot.barh(figsize=(20,12),title=\"Feature Importance\", legend=False);"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
